# Post-hoc Analysis of Trained Classification for Interpretability (2021 Master Thesis)

ABSTRACT: Training neural network requires careful understanding of how the training process is overall processed.
Relying on highly capable computers and large and diverse dataset have shown us the promising potential of
training deep neural network, and results showed superiority of artificial intelligence over human brain. How-
ever, the reasoning behind the decision made by the model is often neglected. In convolutional neural net-
work, several approaches are made in the past to investigate the problem of model interpretability, studying
the convolutional filters during the training process to understand what each layer are converging towards.
The observation of the validation accuracy of the model by training neural network with adaptation on the
curriculum, we demonstrate insights which can lead to better heuristics in training neural networks, which
leads to more efficient and transparent methodologies in machine learning.

Keywords: Deep Learning, Convolutional Neural Network, Curriculum Learning, Model Interpretability
