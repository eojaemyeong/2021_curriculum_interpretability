# Post-hoc Analysis of Trained Classification for Interpretability (2021 Master Thesis)

ABSTRACT: Training neural network requires careful understanding of how the training process is overall processed.
Relying on highly capable computers and large and diverse dataset have shown us the promising potential of
training deep neural network, and results showed superiority of artificial intelligence over human brain. How-
ever, the reasoning behind the decision made by the model is often neglected. In convolutional neural net-
work, several approaches are made in the past to investigate the problem of model interpretability, studying
the convolutional filters during the training process to understand what each layer are converging towards.
The observation of the validation accuracy of the model by training neural network with adaptation on the
curriculum, we demonstrate insights which can lead to better heuristics in training neural networks, which
leads to more efficient and transparent methodologies in machine learning.

Keywords: Deep Learning, Convolutional Neural Network, Curriculum Learning, Model Interpretability


설명 가능한 심층신경망 모델학습 방법론 분석

심층신경망 모델의 학습에는 세부적인 asdf 모델구조의 설계, 그리고 학습데이터에 따른 학습과정에 대한 관찰이 필요하다. 고성능 컴퓨터의 대중화, 그리고 빅데이터에 접근성 향상의 결과로 심층 뉴런 모델은 인류의 두뇌보다 높은 성능을 보여주었다. 다만 모델의 학습과정과 결정에 대한 설명의 필요성은 소홀히 관찰되었고, 최근 설명 가능한 모델에 필요성에 대한 많은 연구가 진행되고 있니다. 컨벌루션 뉴럴 네트워크의 경우, 학습 과정에 레이어의 필터값의 관찰을 통하여 모델의 추론의 논리를 이해하는 연구, 그리고 모델의 학습에 있어 필요한 데이터의 순서를 임의로 정하지 않고, 데이터의 복잡성에 따라 순차적으로 학습시키는 기술이 제시되었다. 본 연구는 학습된 심층신경망 모델의 학습과정 관찰을 통하여 모델의 커리큘럼을 설계하고 커리큘럼에 따라 학습된 모델의 정확도를 분석한다. 이를 통하여 모델의 학습과정에 대하여 연구자들이 이해 가능한 모델 학습 방법론의 타당성을 분석한다.

핵심어: 딥 러닝, 컨벌루션 뉴럴 네트워크, 설명 가능한 모델, 커리큘럼 학습
