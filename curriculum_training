import pickle
import matplotlib.pyplot as plt
import keras
from keras import backend as K
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import np_utils
from keras.layers.normalization import BatchNormalization
from keras.layers import Dense, Dropout, Activation, Flatten

#training batch size
bs = 32
#epoch for each batch
ep = 10
# the order of batches being trained
curriculum = [1,2,3,4,5]

def train_data_unpickle(data_directory, batch_number):
    with open(data_directory + '/data_batch_' + str(batch_number), mode='rb') as each:
        batch_ = pickle.load(each, encoding='latin1')
        # batch_label = batch_['batch_label']
        data = batch_['data'].reshape((len(batch_['data']), 3, 32, 32)).transpose(0, 2, 3, 1)
        # filenames = batch_['filenames']
        labels = batch_['labels']
    return data, labels
    # return batch_label, data, filenames, labels
# returns data, labels

def test__data_unpickle(data_directory):
    with open(data_directory + '/test_batch', mode='rb') as each:
        batch_ = pickle.load(each, encoding='latin1')
        # batch_label = batch_['batch_label']
        data = batch_['data'].reshape((len(batch_['data']), 3, 32, 32)).transpose(0, 2, 3, 1)
        # filenames = batch_['filenames']
        labels = batch_['labels']
    return data, labels
    # return batch_label, data, filenames, labels
# returns data, labels

def reading_train_data (data_directory, batch_number):
    train_data, train_labels = train_data_unpickle(data_directory, batch_number)
    train_labels = np_utils.to_categorical (train_labels)
    return train_data, train_labels
# returns train_data, train_labels

def reading_test__data (data_directory):
    test__data, test__labels = test__data_unpickle(data_directory)
    test__labels = np_utils.to_categorical(test__labels)
    return test__data, test__labels
# returns test__data, test__labels

def training_model(load, P_train_data, P_train_labels, P_test__data, P_test__labels):
    model = Sequential()
    if load == 0:
        model.add(Conv2D(32, (3, 3), padding='same', input_shape=P_train_data.shape[1:]))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(Conv2D(64, (3, 3)))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.25))
        model.add(Conv2D(128, (3, 3), padding='same'))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(Conv2D(64, (3, 3)))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.25))
         
        model.add(Flatten())
        model.add(Dense(512))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(Dropout(0.5))
        model.add(Dense(10))
        model.add(Activation('softmax'))
        
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        K.set_value(model.optimizer.learning_rate, 0.001) 
        P_train_data = P_train_data.astype('float32')
        P_test__data = P_test__data.astype('float32')
        P_train_data /= 255
        P_test__data /= 255
         
        hist = model.fit(P_train_data, P_train_labels, validation_data=(P_test__data, P_test__labels), batch_size = bs, epochs=ep)
        
        model.save("/home/eoj/Documents/model/without_curriculum.h5")
        return model, hist
    else:
        model = keras.models.load_model("/home/eoj/Documents/model/without_curriculum.h5")
        hist = model.fit(train_data, train_labels, validation_data=(test__data, test__labels), batch_size = bs, epochs=ep)
        model.save("/home/eoj/Documents/model/without_curriculum.h5")
        return model, hist
# returns model, hist

def model_evaluation( batch_number, hist, test__data, test__labels ):
    scores = model.evaluate(test__data, test__labels, verbose=0)
    print("\nBatch number " + str(batch_number) + ": CNN Error: %.2f%%" % (100-scores[1]*100))
     
    fig, acc_ax = plt.subplots()
    
    acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')
    
    acc_ax.set_xlabel('epoch')
    acc_ax.set_ylabel('accuracy')
    
    acc_ax.legend(loc='lower left')
    plt.ylim(0,1) 
    plt.show()

    test__data, test__labels = reading_test__data(
        '/home/eoj/Documents/data/cifar-10-batches-py')

    train_data, train_labels = reading_train_data(
        '/home/eoj/Documents/data/cifar-10-batches-py/', curriculum[0])
    model, hist = training_model(0, train_data, train_labels, test__data, test__labels)
    model_evaluation(curriculum[0] ,hist, test__data, test__labels)

    train_data, train_labels = reading_train_data(
        '/home/eoj/Documents/data/cifar-10-batches-py/', curriculum[1])
    model, hist = training_model(-1, train_data, train_labels, test__data, test__labels)
    model_evaluation(curriculum[1] ,hist, test__data, test__labels)

    train_data, train_labels = reading_train_data(
        '/home/eoj/Documents/data/cifar-10-batches-py/', curriculum[2])
    model, hist = training_model(-1, train_data, train_labels, test__data, test__labels)
    model_evaluation(curriculum[2],hist, test__data, test__labels)

    train_data, train_labels = reading_train_data(
        '/home/eoj/Documents/data/cifar-10-batches-py/',curriculum[3])
    model, hist = training_model(-1, train_data, train_labels, test__data, test__labels)
    model_evaluation(curriculum[3], hist, test__data, test__labels)

    train_data, train_labels = reading_train_data(
        '/home/eoj/Documents/data/cifar-10-batches-py/',curriculum[4])
    model, hist = training_model(-1, train_data, train_labels, test__data, test__labels)
    model_evaluation(curriculum[4], hist, test__data, test__labels)








